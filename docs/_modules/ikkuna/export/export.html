<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>ikkuna.export.export &#8212; ikkuna 0 documentation</title>
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html"><span><img src="../../../_static/logo.png"></span>
          ikkuna</a>
        <span class="navbar-text navbar-version pull-left"><b>0.0.2</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide.html">User Guide</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../ikkuna.html">ikkuna</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../main.html">main program</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../train.html">train</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
              
                
              
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <h1>Source code for ikkuna.export.export</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">ikkuna.export.messages</span> <span class="k">import</span> <span class="n">get_default_bus</span>
<span class="kn">from</span> <span class="nn">ikkuna.utils</span> <span class="k">import</span> <span class="n">ModuleTree</span>
<span class="kn">from</span> <span class="nn">ikkuna.utils</span> <span class="k">import</span> <span class="n">freeze_module</span>


<div class="viewcode-block" id="Exporter"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter">[docs]</a><span class="k">class</span> <span class="nc">Exporter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Class for managing publishing of data from model code.</span>

<span class="sd">    An :class:`Exporter` is used in the model code by either explicitly registering modules for</span>
<span class="sd">    tracking with :meth:`~Exporter.add_modules()` or by calling it with newly constructed modules</span>
<span class="sd">    which will then be returned as-is, but be registered in the process.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        e = Exporter(...)</span>
<span class="sd">        features = nn.Sequential([</span>
<span class="sd">            nn.Linear(...),</span>
<span class="sd">            e(nn.Conv2d(...)),</span>
<span class="sd">            nn.ReLU()</span>
<span class="sd">        ])</span>

<span class="sd">    Modules will be tracked recursively unless specified otherwise, meaning the following is</span>
<span class="sd">    possible:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        e = Exporter(...)</span>
<span class="sd">        e.add_modules(extremely_complex_model)</span>
<span class="sd">        # e will now track all layers of extremely_complex_model</span>

<span class="sd">    Three further changes to the training code are necessary</span>

<span class="sd">        #. :meth:`~Exporter.set_model()` to have the :class:`Exporter` wire up the appropriate</span>
<span class="sd">           callbacks.</span>
<span class="sd">        #. :meth:`~Exporter.set_loss()` should be called with the loss function so that</span>
<span class="sd">           labels can be extracted during training if if any</span>
<span class="sd">           :class:`~ikkuna.export.subscriber.Subscriber`\ s rely on the ``&#39;input_labels&#39;`` message</span>
<span class="sd">        #. :meth:`~Exporter.epoch_finished()` should be called if any</span>
<span class="sd">           :class:`~ikkuna.export.subscriber.Subscriber`\ s rely on the ``&#39;epoch_finished&#39;`` message</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    _modules    :   dict(torch.nn.Module, ikkuna.utils.NamedModule)</span>
<span class="sd">                    All tracked modules</span>
<span class="sd">    _weight_cache   :   dict</span>
<span class="sd">                        Cache for keeping the previous weights for computing differences</span>
<span class="sd">    _bias_cache :   dict</span>
<span class="sd">                    see ``_weight_cache``</span>
<span class="sd">    _model          :   torch.nn.Module</span>
<span class="sd">    _train_step :   int</span>
<span class="sd">                    Current batch index</span>
<span class="sd">    _global_step    :   int</span>
<span class="sd">                        Global step accross all epochs</span>
<span class="sd">    _epoch  :   int</span>
<span class="sd">                Current epoch</span>
<span class="sd">    _is_training    :   bool</span>
<span class="sd">                        Flag enabling/disabling some messages during testing</span>
<span class="sd">    _depth  :   int</span>
<span class="sd">                Depth to which to traverse the module tree</span>
<span class="sd">    _module_filter  :   list(torch.nn.Module)</span>
<span class="sd">                        Set of modules to capture when calling :meth:`add_modules()`. Everything not</span>
<span class="sd">                        in this list is ignored</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">module_filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">message_bus</span><span class="o">=</span><span class="n">get_default_bus</span><span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span>           <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span>      <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span>        <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>             <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span>             <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># for gradient and activation to have the same step number, we need to increase it before</span>
        <span class="c1"># propagation or after backpropagation. but we don&#39;t know when the backprop finishes, while</span>
        <span class="c1"># we do know when the forward prop starts. So we step before and thus initialize the</span>
        <span class="c1"># counters with -1 to effectively start at 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span>        <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span>       <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_training</span>       <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_depth</span>             <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frozen</span>            <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_filter</span>     <span class="o">=</span> <span class="n">module_filter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span>           <span class="o">=</span> <span class="n">message_bus</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span> <span class="o">=</span> <span class="s1">&#39;default&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">message_bus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;list(torch.nn.Module) - Modules tracked by this :class:`Exporter`&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;list(ikkuna.utils.NamedModule) - Named modules tracked by this :class:`Exporter`&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<div class="viewcode-block" id="Exporter._add_module_by_name"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter._add_module_by_name">[docs]</a>    <span class="k">def</span> <span class="nf">_add_module_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">named_module</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Register a module with a name attached.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        named_module    :   ikkuna.utils.NamedModule</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">module</span>                <span class="o">=</span> <span class="n">named_module</span><span class="o">.</span><span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">named_module</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_activations</span><span class="p">)</span>

        <span class="c1"># for a new module, immediately cache the weights and biases. This is necessary, because</span>
        <span class="c1"># weights and updates need to be published in step() as only at the end of a batch (here the</span>
        <span class="c1"># beginning of the next one) the updates can be computed. but since we call step before the</span>
        <span class="c1"># forward pass, the new_activations() method has had no chance to cache the current weights</span>
        <span class="c1"># before the first batch starts. so we do it here.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">def</span> <span class="nf">layer_grad_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_layer_gradients</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span>

        <span class="n">module</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">layer_grad_hook</span><span class="p">)</span>

        <span class="n">has_bias</span>   <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">has_weight</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_weight</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_bias</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># For some reason, registered tensor hooks are called twice in my setup. Maybe this means</span>
        <span class="c1"># that the gradient is computed twice, because the grad tensors are identical. Not sure why</span>
        <span class="c1"># this is so.</span>
        <span class="c1"># cache weight and bias gradients and only call new_parameter_gradients when both are</span>
        <span class="c1"># received</span>
        <span class="n">grad_cache</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>

        <span class="c1"># the hooks will check whether both weight and bias have been received and if so, trigger</span>
        <span class="c1"># publication. If the module has no bias, then ``None`` is published for the bias component.</span>
        <span class="c1"># They also check whether we grads were already published at this train step and do nothing</span>
        <span class="c1"># in that case.</span>
        <span class="k">def</span> <span class="nf">weight_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Already received weight gradients for </span><span class="si">{named_module.name}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_bias</span> <span class="ow">or</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_parameter_gradients</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]))</span>
                <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">def</span> <span class="nf">bias_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Already received bias gradients for </span><span class="si">{named_module.name}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_bias</span> <span class="ow">or</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_parameter_gradients</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]))</span>
                <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">bias_hook</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">weight_hook</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.add_modules"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.add_modules">[docs]</a>    <span class="k">def</span> <span class="nf">add_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Add modules to supervise. If the module has ``weight`` and/or ``bias`` members, updates</span>
<span class="sd">        to those will be tracked. Ignores any module in :attr:`_module_filter`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   tuple(str, torch.nn.Module) or torch.nn.Module</span>
<span class="sd">        recursive   :   bool</span>
<span class="sd">                        Descend recursively into the module tree</span>
<span class="sd">        depth   :   int</span>
<span class="sd">                    Depth to which to traverse the tree. Modules below this level will be ignored</span>
<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If ``module`` is neither a tuple, nor a (subclass of) :class:`torch.nn.Module`</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>   <span class="c1"># name already given -&gt; use that</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="n">module_tree</span> <span class="o">=</span> <span class="n">ModuleTree</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="n">recursive</span><span class="p">,</span> <span class="n">drop_name</span><span class="o">=</span><span class="n">recursive</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">named_module</span> <span class="ow">in</span> <span class="n">module_tree</span><span class="o">.</span><span class="n">preorder</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_depth</span><span class="p">):</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">named_module</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_filter</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_filter</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Adding &#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_add_module_by_name</span><span class="p">(</span><span class="n">named_module</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skipping &#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Don</span><span class="se">\&#39;</span><span class="s1">t know how to handle </span><span class="si">{module.__class__.__name__}</span><span class="s1">&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.__call__"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.__call__">[docs]</a>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Shorthand for :meth:`~Exporter.add_modules()` which returns its input unmodified.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        see :meth:`Exporter.add_modules()`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.nn.Module</span>
<span class="sd">            The input ``module``</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_modules</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">recursive</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span></div>

<div class="viewcode-block" id="Exporter.train"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Switch to training mode. This will ensure all data is published.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_training</span> <span class="o">=</span> <span class="n">train</span></div>

<div class="viewcode-block" id="Exporter.test"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.test">[docs]</a>    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Switch to testing mode. This will turn off all publishing.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="ow">not</span> <span class="n">test</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_loss"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_loss">[docs]</a>    <span class="k">def</span> <span class="nf">new_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for publishing current training loss.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_input_data"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_input_data">[docs]</a>    <span class="k">def</span> <span class="nf">new_input_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for new training input to the network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *args   :   tuple</span>
<span class="sd">                    Network inputs</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">args</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;input_data&#39;</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_output_and_labels"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_output_and_labels">[docs]</a>    <span class="k">def</span> <span class="nf">new_output_and_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for final network output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        data    :   torch.Tensor</span>
<span class="sd">                    The final layer&#39;s output</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;network_output&#39;</span><span class="p">,</span> <span class="n">network_output</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;input_labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_activations"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_activations">[docs]</a>    <span class="k">def</span> <span class="nf">new_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">in_</span><span class="p">,</span> <span class="n">out_</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for newly arriving activations. Registered as a hook to the tracked modules.</span>
<span class="sd">        Will trigger export of all new activation and weight/bias data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   torch.nn.Module</span>
<span class="sd">        in_ :   torch.Tensor</span>
<span class="sd">                Dunno what this is</span>
<span class="sd">        out_    :   torch.Tensor</span>
<span class="sd">                    The new activations</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span> <span class="s1">&#39;epoch_started&#39;</span><span class="p">,</span>
                                                  <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># save weights and biases to publish just before current step ends (in step()). this ensures</span>
        <span class="c1"># we can publish the proper updates</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>   <span class="c1"># bias can be present, but be None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                             <span class="s1">&#39;activations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">out_</span><span class="p">,</span>
                                             <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_layer_gradients"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_layer_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">new_layer_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for newly arriving layer gradients (loss wrt layer output). Registered as a hook</span>
<span class="sd">        to the tracked modules.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            Currently, only layers with one output are supported. It&#39;s not clear to me how one layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   torch.nn.Module</span>
<span class="sd">        gradients    :  torch.Tensor</span>
<span class="sd">                        The gradients of the loss w.r.t. layer output</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            If the module has multiple outputs</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Layers with more than one output are not supported.&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                             <span class="s1">&#39;layer_gradients&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">gradients</span><span class="p">,</span>
                                             <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_parameter_gradients"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_parameter_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">new_parameter_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for newly arriving gradients wrt weight and/or bias. Registered as a hook to the</span>
<span class="sd">        tracked modules.  Will trigger export of all new gradient data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   torch.nn.Module</span>
<span class="sd">        gradients    :   tuple(torch.Tensor, torch.Tensor)</span>
<span class="sd">                        The gradients w.r.t weight and bias.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                             <span class="s1">&#39;weight_gradients&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                             <span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                             <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                 <span class="s1">&#39;bias_gradients&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                                 <span class="n">gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.set_model"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.set_model">[docs]</a>    <span class="k">def</span> <span class="nf">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Set the model for direct access for some metrics.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model   :   torch.nn.Module</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="kn">from</span> <span class="nn">types</span> <span class="k">import</span> <span class="n">MethodType</span>
        <span class="c1">#############################################</span>
        <span class="c1">#  Patch the train function to notify self  #</span>
        <span class="c1">#############################################</span>
        <span class="n">train_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span>

        <span class="k">def</span> <span class="nf">new_train_fn</span><span class="p">(</span><span class="n">this</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">train_fn</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_training</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">new_train_fn</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

        <span class="c1">#########################################################</span>
        <span class="c1">#  Patch forward function to step() self automatically  #</span>
        <span class="c1">#########################################################</span>
        <span class="n">forward_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span>

        <span class="k">def</span> <span class="nf">new_forward_fn</span><span class="p">(</span><span class="n">this</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">should_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39;When subscribers need to push data through the net, they should be given access to</span>
<span class="sd">            model.forward() to control the Exporter&#39;s behaviour until their compute() method ends.</span>
<span class="sd">            The `should_train` parameter can be used to temporarily have the model in validation</span>
<span class="sd">            mode. The `tag` parameter can be used to temporarily have all messages be published with</span>
<span class="sd">            a different tag.</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="n">previous_tag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span> <span class="o">=</span> <span class="n">tag</span>
            <span class="c1"># In order for accuracy subscribers to not need the model access, we add a secret</span>
            <span class="c1"># parameter which they can use to temporarily set the training to False and have it</span>
            <span class="c1"># revert automatically. TODO: Check if this is inefficient</span>
            <span class="n">was_training</span> <span class="o">=</span> <span class="n">this</span><span class="o">.</span><span class="n">training</span>        <span class="c1"># store old value</span>
            <span class="n">this</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">should_train</span><span class="p">)</span>            <span class="c1"># disable/enable training</span>
            <span class="k">if</span> <span class="n">this</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="c1"># we need to step before forward pass, else act and grads get different steps</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_input_data</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>               <span class="c1"># do this after stepping</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">forward_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>             <span class="c1"># do forward pass w/o messages spawning</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span> <span class="o">=</span> <span class="n">previous_tag</span>
            <span class="n">this</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">was_training</span><span class="p">)</span>            <span class="c1"># restore previous state</span>
            <span class="k">return</span> <span class="n">ret</span>
        <span class="n">model</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">new_forward_fn</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.set_loss"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.set_loss">[docs]</a>    <span class="k">def</span> <span class="nf">set_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Add hook to loss function to extract labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loss_function   :   torch.nn._Loss</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">output_and_labels</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
            <span class="n">network_output</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">output_and_labels</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_output_and_labels</span><span class="p">(</span><span class="n">network_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">loss_function</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.step"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.step">[docs]</a>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Increase batch counter (per epoch) and the global step counter.&#39;&#39;&#39;</span>
        <span class="c1"># due to the fact that backprop happens after forward() was called, we need to step before</span>
        <span class="c1"># the forward pass so activation and gradient msgs have the same counter. therefore, the</span>
        <span class="c1"># counters start at -1, but we publish a &#39;batch_finished&#39; message only from the second</span>
        <span class="c1"># iteration onwards</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                  <span class="s1">&#39;batch_finished&#39;</span><span class="p">,</span>
                                                  <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span>  <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span> <span class="s1">&#39;weight_updates&#39;</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                                 <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">-</span> <span class="n">weight</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                 <span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">weight</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">bias</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span> <span class="s1">&#39;bias_updates&#39;</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                                 <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="n">bias</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                 <span class="s1">&#39;biases&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">bias</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;batch_started&#39;</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.freeze_module"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.freeze_module">[docs]</a>    <span class="k">def</span> <span class="nf">freeze_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Convenience method for freezing training for a module.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module    :   torch.nn.Module</span>
<span class="sd">                      Module to freeze</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">if</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_frozen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Freezing </span><span class="si">{module}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">freeze_module</span><span class="p">(</span><span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.epoch_finished"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.epoch_finished">[docs]</a>    <span class="k">def</span> <span class="nf">epoch_finished</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Increase the epoch counter and reset the batch counter.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;epoch_finished&#39;</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span>     <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span> <span class="o">=</span> <span class="kc">False</span></div></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2018, Rasmus Diederichsen.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>