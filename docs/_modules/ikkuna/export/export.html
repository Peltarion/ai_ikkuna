

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ikkuna.export.export &mdash; ikkuna 0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> ikkuna
          

          
            
            <img src="../../../_static/ikkuna_logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.0post2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide.html">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide.html#installing-the-library">Installing the library</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide.html#reporting-issues">Reporting Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide.html#quickstart">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide.html#details">Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide.html#creating-a-new-subscriber">Creating a new Subscriber</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide.html#installing-the-subscriber">Installing the Subscriber</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../ikkuna.html">ikkuna</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ikkuna.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ikkuna.export.html">ikkuna.export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.export.html#module-ikkuna.export">Module Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.export.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.export.html#subpackages">Subpackages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../ikkuna.utils.html">ikkuna.utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.utils.html#module-ikkuna.utils">Module contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.utils.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../ikkuna.visualization.html">ikkuna.visualization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.visualization.html#module-ikkuna.visualization">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../ikkuna.models.html">ikkuna.models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ikkuna.models.html#module-ikkuna.models">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../main.html">main program</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../main.html#Named Arguments">Named Arguments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../train.html">train</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../train.html#module-train">Module Contents</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">ikkuna</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>ikkuna.export.export</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for ikkuna.export.export</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">ikkuna.export.messages</span> <span class="k">import</span> <span class="n">get_default_bus</span>
<span class="kn">from</span> <span class="nn">ikkuna.utils</span> <span class="k">import</span> <span class="n">ModuleTree</span>
<span class="kn">from</span> <span class="nn">ikkuna.utils</span> <span class="k">import</span> <span class="n">freeze_module</span>


<div class="viewcode-block" id="Exporter"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter">[docs]</a><span class="k">class</span> <span class="nc">Exporter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Class for managing publishing of data from model code.</span>

<span class="sd">    An :class:`Exporter` is used in the model code by either explicitly registering modules for</span>
<span class="sd">    tracking with :meth:`~Exporter.add_modules()` or by calling it with newly constructed modules</span>
<span class="sd">    which will then be returned as-is, but be registered in the process.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        e = Exporter(...)</span>
<span class="sd">        features = nn.Sequential([</span>
<span class="sd">            nn.Linear(...),</span>
<span class="sd">            e(nn.Conv2d(...)),</span>
<span class="sd">            nn.ReLU()</span>
<span class="sd">        ])</span>

<span class="sd">    Modules will be tracked recursively unless specified otherwise, meaning the following is</span>
<span class="sd">    possible:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        e = Exporter(...)</span>
<span class="sd">        e.add_modules(extremely_complex_model)</span>
<span class="sd">        # e will now track all layers of extremely_complex_model</span>

<span class="sd">    Three further changes to the training code are necessary</span>

<span class="sd">        #. :meth:`~Exporter.set_model()` to have the :class:`Exporter` wire up the appropriate</span>
<span class="sd">           callbacks.</span>
<span class="sd">        #. :meth:`~Exporter.set_loss()` should be called with the loss function so that</span>
<span class="sd">           labels can be extracted during training if if any</span>
<span class="sd">           :class:`~ikkuna.export.subscriber.Subscriber`\ s rely on the ``&#39;input_labels&#39;`` message</span>
<span class="sd">        #. :meth:`~Exporter.epoch_finished()` should be called if any</span>
<span class="sd">           :class:`~ikkuna.export.subscriber.Subscriber`\ s rely on the ``&#39;epoch_finished&#39;`` message</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    _modules    :   dict(torch.nn.Module, ikkuna.utils.NamedModule)</span>
<span class="sd">                    All tracked modules</span>
<span class="sd">    _weight_cache   :   dict</span>
<span class="sd">                        Cache for keeping the previous weights for computing differences</span>
<span class="sd">    _bias_cache :   dict</span>
<span class="sd">                    see ``_weight_cache``</span>
<span class="sd">    _model          :   torch.nn.Module</span>
<span class="sd">    _train_step :   int</span>
<span class="sd">                    Current batch index</span>
<span class="sd">    _global_step    :   int</span>
<span class="sd">                        Global step accross all epochs</span>
<span class="sd">    _epoch  :   int</span>
<span class="sd">                Current epoch</span>
<span class="sd">    _is_training    :   bool</span>
<span class="sd">                        Flag enabling/disabling some messages during testing</span>
<span class="sd">    _depth  :   int</span>
<span class="sd">                Depth to which to traverse the module tree</span>
<span class="sd">    _module_filter  :   list(torch.nn.Module)</span>
<span class="sd">                        Set of modules to capture when calling :meth:`add_modules()`. Everything not</span>
<span class="sd">                        in this list is ignored</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">module_filter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">message_bus</span><span class="o">=</span><span class="n">get_default_bus</span><span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span>           <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span>      <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span>        <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>             <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span>             <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># for gradient and activation to have the same step number, we need to increase it before</span>
        <span class="c1"># propagation or after backpropagation. but we don&#39;t know when the backprop finishes, while</span>
        <span class="c1"># we do know when the forward prop starts. So we step before and thus initialize the</span>
        <span class="c1"># counters with -1 to effectively start at 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span>        <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span>       <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_training</span>       <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_depth</span>             <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frozen</span>            <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_filter</span>     <span class="o">=</span> <span class="n">module_filter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span>           <span class="o">=</span> <span class="n">message_bus</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span> <span class="o">=</span> <span class="s1">&#39;default&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">message_bus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;list(torch.nn.Module) - Modules tracked by this :class:`Exporter`&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;list(ikkuna.utils.NamedModule) - Named modules tracked by this :class:`Exporter`&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<div class="viewcode-block" id="Exporter._add_module_by_name"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter._add_module_by_name">[docs]</a>    <span class="k">def</span> <span class="nf">_add_module_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">named_module</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Register a module with a name attached.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        named_module    :   ikkuna.utils.NamedModule</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">module</span>                <span class="o">=</span> <span class="n">named_module</span><span class="o">.</span><span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">named_module</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_activations</span><span class="p">)</span>

        <span class="c1"># for a new module, immediately cache the weights and biases. This is necessary, because</span>
        <span class="c1"># weights and updates need to be published in step() as only at the end of a batch (here the</span>
        <span class="c1"># beginning of the next one) the updates can be computed. but since we call step before the</span>
        <span class="c1"># forward pass, the new_activations() method has had no chance to cache the current weights</span>
        <span class="c1"># before the first batch starts. so we do it here.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">def</span> <span class="nf">layer_grad_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_layer_gradients</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span>

        <span class="n">module</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">layer_grad_hook</span><span class="p">)</span>

        <span class="n">has_bias</span>   <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">has_weight</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_weight</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_bias</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># For some reason, registered tensor hooks are called twice in my setup. Maybe this means</span>
        <span class="c1"># that the gradient is computed twice, because the grad tensors are identical. Not sure why</span>
        <span class="c1"># this is so.</span>
        <span class="c1"># cache weight and bias gradients and only call new_parameter_gradients when both are</span>
        <span class="c1"># received</span>
        <span class="n">grad_cache</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>

        <span class="c1"># the hooks will check whether both weight and bias have been received and if so, trigger</span>
        <span class="c1"># publication. If the module has no bias, then ``None`` is published for the bias component.</span>
        <span class="c1"># They also check whether we grads were already published at this train step and do nothing</span>
        <span class="c1"># in that case.</span>
        <span class="k">def</span> <span class="nf">weight_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Already received weight gradients for </span><span class="si">{named_module.name}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_bias</span> <span class="ow">or</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_parameter_gradients</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]))</span>
                <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">def</span> <span class="nf">bias_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Already received bias gradients for </span><span class="si">{named_module.name}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_bias</span> <span class="ow">or</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_parameter_gradients</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]))</span>
                <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_cache</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">bias_hook</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">weight_hook</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.add_modules"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.add_modules">[docs]</a>    <span class="k">def</span> <span class="nf">add_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Add modules to supervise. If the module has ``weight`` and/or ``bias`` members, updates</span>
<span class="sd">        to those will be tracked. Ignores any module in :attr:`_module_filter`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   tuple(str, torch.nn.Module) or torch.nn.Module</span>
<span class="sd">        recursive   :   bool</span>
<span class="sd">                        Descend recursively into the module tree</span>
<span class="sd">        depth   :   int</span>
<span class="sd">                    Depth to which to traverse the tree. Modules below this level will be ignored</span>
<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If ``module`` is neither a tuple, nor a (subclass of) :class:`torch.nn.Module`</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>   <span class="c1"># name already given -&gt; use that</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="n">module_tree</span> <span class="o">=</span> <span class="n">ModuleTree</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="n">recursive</span><span class="p">,</span> <span class="n">drop_name</span><span class="o">=</span><span class="n">recursive</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">named_module</span> <span class="ow">in</span> <span class="n">module_tree</span><span class="o">.</span><span class="n">preorder</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_depth</span><span class="p">):</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">named_module</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_filter</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_filter</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Adding &#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_add_module_by_name</span><span class="p">(</span><span class="n">named_module</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skipping &#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Don</span><span class="se">\&#39;</span><span class="s1">t know how to handle </span><span class="si">{module.__class__.__name__}</span><span class="s1">&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.__call__"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.__call__">[docs]</a>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Shorthand for :meth:`~Exporter.add_modules()` which returns its input unmodified.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        see :meth:`Exporter.add_modules()`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.nn.Module</span>
<span class="sd">            The input ``module``</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_modules</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">recursive</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span></div>

<div class="viewcode-block" id="Exporter.train"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Switch to training mode. This will ensure all data is published.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_training</span> <span class="o">=</span> <span class="n">train</span></div>

<div class="viewcode-block" id="Exporter.test"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.test">[docs]</a>    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Switch to testing mode. This will turn off all publishing.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="ow">not</span> <span class="n">test</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_loss"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_loss">[docs]</a>    <span class="k">def</span> <span class="nf">new_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for publishing current training loss.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_input_data"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_input_data">[docs]</a>    <span class="k">def</span> <span class="nf">new_input_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for new training input to the network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *args   :   tuple</span>
<span class="sd">                    Network inputs</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="n">args</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;input_data&#39;</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_output_and_labels"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_output_and_labels">[docs]</a>    <span class="k">def</span> <span class="nf">new_output_and_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for final network output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        data    :   torch.Tensor</span>
<span class="sd">                    The final layer&#39;s output</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;network_output&#39;</span><span class="p">,</span> <span class="n">network_output</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;input_labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_activations"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_activations">[docs]</a>    <span class="k">def</span> <span class="nf">new_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">in_</span><span class="p">,</span> <span class="n">out_</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for newly arriving activations. Registered as a hook to the tracked modules.</span>
<span class="sd">        Will trigger export of all new activation and weight/bias data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   torch.nn.Module</span>
<span class="sd">        in_ :   torch.Tensor</span>
<span class="sd">                Dunno what this is</span>
<span class="sd">        out_    :   torch.Tensor</span>
<span class="sd">                    The new activations</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span> <span class="s1">&#39;epoch_started&#39;</span><span class="p">,</span>
                                                  <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># save weights and biases to publish just before current step ends (in step()). this ensures</span>
        <span class="c1"># we can publish the proper updates</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>   <span class="c1"># bias can be present, but be None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                             <span class="s1">&#39;activations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">out_</span><span class="p">,</span>
                                             <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_layer_gradients"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_layer_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">new_layer_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for newly arriving layer gradients (loss wrt layer output). Registered as a hook</span>
<span class="sd">        to the tracked modules.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            Currently, only layers with one output are supported.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   torch.nn.Module</span>
<span class="sd">        gradients    :  torch.Tensor</span>
<span class="sd">                        The gradients of the loss w.r.t. layer output</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            If the module has multiple outputs</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Layers with more than one output are not supported.&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                             <span class="s1">&#39;layer_gradients&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">gradients</span><span class="p">,</span>
                                             <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.new_parameter_gradients"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.new_parameter_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">new_parameter_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Callback for newly arriving gradients wrt weight and/or bias. Registered as a hook to the</span>
<span class="sd">        tracked modules.  Will trigger export of all new gradient data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module  :   torch.nn.Module</span>
<span class="sd">        gradients    :   tuple(torch.Tensor, torch.Tensor)</span>
<span class="sd">                        The gradients w.r.t weight and bias.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                             <span class="s1">&#39;weight_gradients&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                             <span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                             <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                 <span class="s1">&#39;bias_gradients&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                                 <span class="n">gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.set_model"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.set_model">[docs]</a>    <span class="k">def</span> <span class="nf">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Set the model for direct access for some metrics.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model   :   torch.nn.Module</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="kn">from</span> <span class="nn">types</span> <span class="k">import</span> <span class="n">MethodType</span>
        <span class="c1">#############################################</span>
        <span class="c1">#  Patch the train function to notify self  #</span>
        <span class="c1">#############################################</span>
        <span class="n">train_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span>

        <span class="k">def</span> <span class="nf">new_train_fn</span><span class="p">(</span><span class="n">this</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">train_fn</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_training</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">new_train_fn</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

        <span class="c1">#########################################################</span>
        <span class="c1">#  Patch forward function to step() self automatically  #</span>
        <span class="c1">#########################################################</span>
        <span class="n">forward_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span>

        <span class="k">def</span> <span class="nf">new_forward_fn</span><span class="p">(</span><span class="n">this</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">should_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39;When subscribers need to push data through the net, they should be given access to</span>
<span class="sd">            model.forward() to control the Exporter&#39;s behaviour until their compute() method ends.</span>
<span class="sd">            The `should_train` parameter can be used to temporarily have the model in validation</span>
<span class="sd">            mode. The `tag` parameter can be used to temporarily have all messages be published with</span>
<span class="sd">            a different tag.</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="n">previous_tag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span> <span class="o">=</span> <span class="n">tag</span>
            <span class="c1"># In order for accuracy subscribers to not need the model access, we add a secret</span>
            <span class="c1"># parameter which they can use to temporarily set the training to False and have it</span>
            <span class="c1"># revert automatically. TODO: Check if this is inefficient</span>
            <span class="n">was_training</span> <span class="o">=</span> <span class="n">this</span><span class="o">.</span><span class="n">training</span>        <span class="c1"># store old value</span>
            <span class="n">this</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">should_train</span><span class="p">)</span>            <span class="c1"># disable/enable training</span>
            <span class="k">if</span> <span class="n">this</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="c1"># we need to step before forward pass, else act and grads get different steps</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_input_data</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>               <span class="c1"># do this after stepping</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">forward_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>             <span class="c1"># do forward pass w/o messages spawning</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span> <span class="o">=</span> <span class="n">previous_tag</span>
            <span class="n">this</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">was_training</span><span class="p">)</span>            <span class="c1"># restore previous state</span>
            <span class="k">return</span> <span class="n">ret</span>
        <span class="n">model</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">new_forward_fn</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.set_loss"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.set_loss">[docs]</a>    <span class="k">def</span> <span class="nf">set_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Add hook to loss function to extract labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loss_function   :   torch.nn._Loss</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">output_and_labels</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
            <span class="n">network_output</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">output_and_labels</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_output_and_labels</span><span class="p">(</span><span class="n">network_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">loss_function</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.step"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.step">[docs]</a>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Increase batch counter (per epoch) and the global step counter.&#39;&#39;&#39;</span>
        <span class="c1"># due to the fact that backprop happens after forward() was called, we need to step before</span>
        <span class="c1"># the forward pass so activation and gradient msgs have the same counter. therefore, the</span>
        <span class="c1"># counters start at -1, but we publish a &#39;batch_finished&#39; message only from the second</span>
        <span class="c1"># iteration onwards</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                  <span class="s1">&#39;batch_finished&#39;</span><span class="p">,</span>
                                                  <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span>  <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_cache</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span> <span class="s1">&#39;weight_updates&#39;</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                                 <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">-</span> <span class="n">weight</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                 <span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">weight</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">bias</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_cache</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span> <span class="s1">&#39;bias_updates&#39;</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span>
                                                 <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="n">bias</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_module_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                                 <span class="s1">&#39;biases&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="p">],</span> <span class="n">bias</span><span class="p">,</span>
                                                 <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;batch_started&#39;</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.freeze_module"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.freeze_module">[docs]</a>    <span class="k">def</span> <span class="nf">freeze_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Convenience method for freezing training for a module.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        module    :   torch.nn.Module</span>
<span class="sd">                      Module to freeze</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">if</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_frozen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Freezing </span><span class="si">{module}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">freeze_module</span><span class="p">(</span><span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="Exporter.epoch_finished"><a class="viewcode-back" href="../../../ikkuna.export.html#ikkuna.export.Exporter.epoch_finished">[docs]</a>    <span class="k">def</span> <span class="nf">epoch_finished</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Increase the epoch counter and reset the batch counter.&#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_msg_bus</span><span class="o">.</span><span class="n">publish_network_message</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span><span class="p">,</span>
                                              <span class="s1">&#39;epoch_finished&#39;</span><span class="p">,</span>
                                              <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_publish_tag</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch</span>     <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_started_marker</span> <span class="o">=</span> <span class="kc">False</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Rasmus Diederichsen

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>